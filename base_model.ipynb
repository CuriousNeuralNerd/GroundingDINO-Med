{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6129e48-1b4b-4082-8c28-3dcf8e4a2034",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa82009e-8253-4542-92b5-f8cfb44c73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a output directory to store the checkpoints of trained model\n",
    "import os\n",
    "os.makedirs(\"/home/km/content/output_base/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f38ef7-4f5e-4bd1-aaed-558932edf094",
   "metadata": {},
   "source": [
    "# 4. Download groundingdino_swint_ogc.pth and bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26a643a-6107-4b08-863b-55e99875a588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-15 23:51:51--  https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/611591640/c4c55fde-97e5-47d9-a2c5-b169832a2fa9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241116T045151Z&X-Amz-Expires=300&X-Amz-Signature=63aa73354b811c91a6244006d1e3f8abb9e7aed8dee1ba9e0f4f36c69233d4b7&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgroundingdino_swinb_cogcoor.pth&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-11-15 23:51:51--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/611591640/c4c55fde-97e5-47d9-a2c5-b169832a2fa9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241116T045151Z&X-Amz-Expires=300&X-Amz-Signature=63aa73354b811c91a6244006d1e3f8abb9e7aed8dee1ba9e0f4f36c69233d4b7&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgroundingdino_swinb_cogcoor.pth&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 938057991 (895M) [application/octet-stream]\n",
      "Saving to: ‘groundingdino_swinb_cogcoor.pth’\n",
      "\n",
      "groundingdino_swinb 100%[===================>] 894.60M   100MB/s    in 9.0s    \n",
      "\n",
      "2024-11-15 23:52:00 (99.1 MB/s) - ‘groundingdino_swinb_cogcoor.pth’ saved [938057991/938057991]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the base model 'groundingdino_swinb_cogcoor.pth'\n",
    "!wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b769f-2279-4963-8bad-d42608b3f6d7",
   "metadata": {},
   "source": [
    "# 5. Replacing whole code of `train_dist.sh` with code below to run on single gpu\n",
    "- Add path of groundingdino_swint_ogc.pth and bert folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca46554d-ef8f-4bbf-81a6-35765eeea86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully replaced the content of /home/km/content/Open-GroundingDino/train_dist.sh\n"
     ]
    }
   ],
   "source": [
    "def replace_file_content(file_path, new_content):\n",
    "    try:\n",
    "        # Open the file in write mode and replace its content\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(new_content)\n",
    "        print(f\"Successfully replaced the content of {file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error occurred while replacing content: {e}\")\n",
    "\n",
    "# Define the new content for the file\n",
    "new_content = \"\"\"\\\n",
    "CFG=$1\n",
    "DATASETS=$2\n",
    "OUTPUT_DIR=$3\n",
    "\n",
    "# Set the environment variable for CUDA\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "python main.py \\\\\n",
    "    --config_file ${CFG} \\\\\n",
    "    --datasets ${DATASETS} \\\\\n",
    "    --output_dir ${OUTPUT_DIR} \\\\\n",
    "    --pretrain_model_path /home/km/content/groundingdino_swinb_cogcoor.pth \\\\\n",
    "    --options text_encoder_type=\"/home/km/content/bert\"\n",
    "\"\"\"\n",
    "\n",
    "# Specify the file path\n",
    "file_path = '/home/km/content/Open-GroundingDino/train_dist.sh'\n",
    "\n",
    "# Call the function to replace the content\n",
    "replace_file_content(file_path, new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1b2e5c-7bf9-4d09-b684-157c727ae894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/km/content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/km/pytorch_env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/km/content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159642f-4ebf-4c01-bbce-b488fb4a8b76",
   "metadata": {},
   "source": [
    "# 6. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc3af2e-21eb-457e-bed5-4b1a53dac9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/km/content/Open-GroundingDino\n",
      "Not using distributed mode\n",
      "Loading config file from /home/km/content/Open-GroundingDino/config/cfg_odvg.py\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,051 | \u001b[34mgit:\n",
      "  sha: 924bb6c4b93cae2dae582e1afaeccd408c72a31d, status: has uncommited changes, branch: main\n",
      "\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,051 | \u001b[34mCommand: main.py --config_file /home/km/content/Open-GroundingDino/config/cfg_odvg.py --datasets /home/km/content/Open-GroundingDino/config/datasets_mixed_odvg.json --output_dir /home/km/content/output_base --pretrain_model_path /home/km/content/groundingdino_swinb_cogcoor.pth --options text_encoder_type=/home/km/content/bert\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,052 | \u001b[34mFull config saved to /home/km/content/output_base/config_args_all.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,052 | \u001b[34mworld size: 1\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,052 | \u001b[34mrank: 0\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,052 | \u001b[34mlocal_rank: 0\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,052 | \u001b[34margs: Namespace(config_file='/home/km/content/Open-GroundingDino/config/cfg_odvg.py', options={'text_encoder_type': '/home/km/content/bert'}, datasets='/home/km/content/Open-GroundingDino/config/datasets_mixed_odvg.json', remove_difficult=False, fix_size=False, output_dir='/home/km/content/output_base', note='', device='cuda', seed=42, resume='', pretrain_model_path='/home/km/content/groundingdino_swinb_cogcoor.pth', finetune_ignore=None, start_epoch=0, eval=False, num_workers=8, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, batch_size=4, modelname='groundingdino', backbone='swin_T_224_1k', position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], enc_layers=6, dec_layers=6, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, num_feature_levels=4, enc_n_points=4, dec_n_points=4, two_stage_type='standard', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, transformer_activation='relu', dec_pred_bbox_embed_share=True, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, dn_label_coef=1.0, dn_bbox_coef=1.0, embed_init_tgt=True, dn_labelbook_size=91, max_text_len=256, text_encoder_type='/home/km/content/bert', use_text_enhancer=True, use_fusion_layer=True, use_checkpoint=True, use_transformer_ckpt=True, use_text_cross_attention=True, text_dropout=0.0, fusion_dropout=0.0, fusion_droppath=0.1, sub_sentence_present=True, max_labels=50, lr=0.0001, backbone_freeze_keywords=None, freeze_keywords=['bert'], lr_backbone=1e-05, lr_backbone_names=['backbone.0', 'bert'], lr_linear_proj_mult=1e-05, lr_linear_proj_names=['ref_point_head', 'sampling_offsets'], weight_decay=0.0001, param_dict_type='ddetr_in_mmdet', ddetr_lr_param=False, epochs=15, lr_drop=4, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[4, 8], frozen_weights=None, dilation=False, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=1.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=2.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, mask_loss_coef=1.0, dice_loss_coef=1.0, focal_alpha=0.25, focal_gamma=2.0, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_class_embed_share=True, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, use_coco_eval=False, label_list=['aortic enlargement', 'atelectasis', 'calcification', 'cardiomegaly', 'consolidation', 'ild', 'infiltration', 'lung opacity', 'nodule mass', 'pleural effusion', 'pleural thickening', 'pneumothorax', 'pulmonary fibrosis', 'other lesion'], dn_scalar=100)\n",
      "\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-11-16 21:45:34,053 | \u001b[34mbuild model ... ...\u001b[0m\n",
      "/home/km/pytorch_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/km/pytorch_env/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "final text_encoder_type: /home/km/content/bert\n",
      "final text_encoder_type: /home/km/content/bert\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-11-16 21:45:34,750 | \u001b[34mbuild model, done.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,751 | \u001b[34mnumber of params:172249090\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,753 | \u001b[34mparams before freezing:\n",
      "{\n",
      "  \"transformer.level_embed\": 1024,\n",
      "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.0.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.0.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.0.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.1.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.1.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.1.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.2.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.2.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.2.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.3.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.3.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.3.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.4.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.4.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.4.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.5.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.5.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.5.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.0.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.0.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.0.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.0.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.0.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.0.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.0.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.0.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.0.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.0.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.0.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.0.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.1.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.1.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.1.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.1.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.1.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.1.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.1.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.1.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.1.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.1.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.1.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.1.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.2.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.2.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.2.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.2.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.2.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.2.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.2.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.2.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.2.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.2.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.2.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.2.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.3.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.3.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.3.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.3.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.3.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.3.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.3.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.3.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.3.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.3.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.3.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.3.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.4.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.4.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.4.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.4.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.4.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.4.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.4.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.4.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.4.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.4.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.4.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.4.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.5.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.5.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.5.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.5.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.5.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.5.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.5.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.5.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.5.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.5.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.5.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.5.norm2.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.0.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.0.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.0.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.1.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.1.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.1.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.2.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.2.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.2.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.3.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.3.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.3.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.4.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.4.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.4.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.5.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.5.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.5.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm3.bias\": 256,\n",
      "  \"transformer.decoder.norm.weight\": 256,\n",
      "  \"transformer.decoder.norm.bias\": 256,\n",
      "  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n",
      "  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n",
      "  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n",
      "  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n",
      "  \"transformer.tgt_embed.weight\": 230400,\n",
      "  \"transformer.enc_output.weight\": 65536,\n",
      "  \"transformer.enc_output.bias\": 256,\n",
      "  \"transformer.enc_output_norm.weight\": 256,\n",
      "  \"transformer.enc_output_norm.bias\": 256,\n",
      "  \"transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n",
      "  \"transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n",
      "  \"transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n",
      "  \"transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n",
      "  \"transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n",
      "  \"transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n",
      "  \"bert.embeddings.word_embeddings.weight\": 23440896,\n",
      "  \"bert.embeddings.position_embeddings.weight\": 393216,\n",
      "  \"bert.embeddings.token_type_embeddings.weight\": 1536,\n",
      "  \"bert.embeddings.LayerNorm.weight\": 768,\n",
      "  \"bert.embeddings.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.0.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.0.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.0.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.0.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.0.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.0.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.0.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.0.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.0.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.0.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.0.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.0.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.0.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.0.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.0.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.0.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.1.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.1.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.1.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.1.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.1.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.1.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.1.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.1.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.1.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.1.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.1.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.1.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.1.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.1.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.1.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.1.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.2.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.2.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.2.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.2.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.2.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.2.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.2.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.2.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.2.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.2.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.2.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.2.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.2.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.2.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.2.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.2.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.3.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.3.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.3.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.3.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.3.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.3.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.3.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.3.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.3.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.3.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.3.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.3.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.3.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.3.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.3.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.3.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.4.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.4.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.4.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.4.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.4.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.4.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.4.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.4.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.4.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.4.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.4.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.4.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.4.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.4.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.4.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.4.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.5.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.5.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.5.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.5.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.5.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.5.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.5.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.5.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.5.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.5.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.5.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.5.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.5.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.5.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.5.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.5.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.6.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.6.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.6.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.6.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.6.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.6.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.6.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.6.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.6.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.6.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.6.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.6.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.6.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.6.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.6.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.6.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.7.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.7.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.7.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.7.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.7.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.7.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.7.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.7.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.7.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.7.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.7.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.7.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.7.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.7.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.7.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.7.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.8.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.8.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.8.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.8.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.8.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.8.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.8.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.8.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.8.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.8.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.8.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.8.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.8.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.8.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.8.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.8.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.9.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.9.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.9.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.9.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.9.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.9.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.9.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.9.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.9.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.9.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.9.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.9.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.9.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.9.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.9.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.9.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.10.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.10.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.10.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.10.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.10.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.10.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.10.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.10.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.10.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.10.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.10.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.10.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.10.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.10.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.10.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.10.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.11.attention.self.query.weight\": 589824,\n",
      "  \"bert.encoder.layer.11.attention.self.query.bias\": 768,\n",
      "  \"bert.encoder.layer.11.attention.self.key.weight\": 589824,\n",
      "  \"bert.encoder.layer.11.attention.self.key.bias\": 768,\n",
      "  \"bert.encoder.layer.11.attention.self.value.weight\": 589824,\n",
      "  \"bert.encoder.layer.11.attention.self.value.bias\": 768,\n",
      "  \"bert.encoder.layer.11.attention.output.dense.weight\": 589824,\n",
      "  \"bert.encoder.layer.11.attention.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.11.attention.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.11.attention.output.LayerNorm.bias\": 768,\n",
      "  \"bert.encoder.layer.11.intermediate.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.11.intermediate.dense.bias\": 3072,\n",
      "  \"bert.encoder.layer.11.output.dense.weight\": 2359296,\n",
      "  \"bert.encoder.layer.11.output.dense.bias\": 768,\n",
      "  \"bert.encoder.layer.11.output.LayerNorm.weight\": 768,\n",
      "  \"bert.encoder.layer.11.output.LayerNorm.bias\": 768,\n",
      "  \"feat_map.weight\": 196608,\n",
      "  \"feat_map.bias\": 256,\n",
      "  \"input_proj.0.0.weight\": 49152,\n",
      "  \"input_proj.0.0.bias\": 256,\n",
      "  \"input_proj.0.1.weight\": 256,\n",
      "  \"input_proj.0.1.bias\": 256,\n",
      "  \"input_proj.1.0.weight\": 98304,\n",
      "  \"input_proj.1.0.bias\": 256,\n",
      "  \"input_proj.1.1.weight\": 256,\n",
      "  \"input_proj.1.1.bias\": 256,\n",
      "  \"input_proj.2.0.weight\": 196608,\n",
      "  \"input_proj.2.0.bias\": 256,\n",
      "  \"input_proj.2.1.weight\": 256,\n",
      "  \"input_proj.2.1.bias\": 256,\n",
      "  \"input_proj.3.0.weight\": 1769472,\n",
      "  \"input_proj.3.0.bias\": 256,\n",
      "  \"input_proj.3.1.weight\": 256,\n",
      "  \"input_proj.3.1.bias\": 256,\n",
      "  \"backbone.0.patch_embed.proj.weight\": 4608,\n",
      "  \"backbone.0.patch_embed.proj.bias\": 96,\n",
      "  \"backbone.0.patch_embed.norm.weight\": 96,\n",
      "  \"backbone.0.patch_embed.norm.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.norm1.weight\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.norm1.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.relative_position_bias_table\": 507,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.qkv.weight\": 27648,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.qkv.bias\": 288,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.proj.weight\": 9216,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.proj.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.norm2.weight\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.norm2.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.mlp.fc1.weight\": 36864,\n",
      "  \"backbone.0.layers.0.blocks.0.mlp.fc1.bias\": 384,\n",
      "  \"backbone.0.layers.0.blocks.0.mlp.fc2.weight\": 36864,\n",
      "  \"backbone.0.layers.0.blocks.0.mlp.fc2.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.norm1.weight\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.norm1.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.relative_position_bias_table\": 507,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.qkv.weight\": 27648,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.qkv.bias\": 288,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.proj.weight\": 9216,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.proj.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.norm2.weight\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.norm2.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.mlp.fc1.weight\": 36864,\n",
      "  \"backbone.0.layers.0.blocks.1.mlp.fc1.bias\": 384,\n",
      "  \"backbone.0.layers.0.blocks.1.mlp.fc2.weight\": 36864,\n",
      "  \"backbone.0.layers.0.blocks.1.mlp.fc2.bias\": 96,\n",
      "  \"backbone.0.layers.0.downsample.reduction.weight\": 73728,\n",
      "  \"backbone.0.layers.0.downsample.norm.weight\": 384,\n",
      "  \"backbone.0.layers.0.downsample.norm.bias\": 384,\n",
      "  \"backbone.0.layers.1.blocks.0.norm1.weight\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.norm1.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.relative_position_bias_table\": 1014,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.qkv.weight\": 110592,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.qkv.bias\": 576,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.proj.weight\": 36864,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.proj.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.norm2.weight\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.norm2.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.mlp.fc1.weight\": 147456,\n",
      "  \"backbone.0.layers.1.blocks.0.mlp.fc1.bias\": 768,\n",
      "  \"backbone.0.layers.1.blocks.0.mlp.fc2.weight\": 147456,\n",
      "  \"backbone.0.layers.1.blocks.0.mlp.fc2.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.norm1.weight\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.norm1.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.relative_position_bias_table\": 1014,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.qkv.weight\": 110592,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.qkv.bias\": 576,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.proj.weight\": 36864,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.proj.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.norm2.weight\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.norm2.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.mlp.fc1.weight\": 147456,\n",
      "  \"backbone.0.layers.1.blocks.1.mlp.fc1.bias\": 768,\n",
      "  \"backbone.0.layers.1.blocks.1.mlp.fc2.weight\": 147456,\n",
      "  \"backbone.0.layers.1.blocks.1.mlp.fc2.bias\": 192,\n",
      "  \"backbone.0.layers.1.downsample.reduction.weight\": 294912,\n",
      "  \"backbone.0.layers.1.downsample.norm.weight\": 768,\n",
      "  \"backbone.0.layers.1.downsample.norm.bias\": 768,\n",
      "  \"backbone.0.layers.2.blocks.0.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.0.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.0.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.0.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.1.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.1.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.1.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.2.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.2.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.2.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.3.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.3.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.3.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.4.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.4.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.4.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.5.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.5.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.5.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.downsample.reduction.weight\": 1179648,\n",
      "  \"backbone.0.layers.2.downsample.norm.weight\": 1536,\n",
      "  \"backbone.0.layers.2.downsample.norm.bias\": 1536,\n",
      "  \"backbone.0.layers.3.blocks.0.norm1.weight\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.norm1.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.relative_position_bias_table\": 4056,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.qkv.weight\": 1769472,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.qkv.bias\": 2304,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.proj.weight\": 589824,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.proj.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.norm2.weight\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.norm2.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.mlp.fc1.weight\": 2359296,\n",
      "  \"backbone.0.layers.3.blocks.0.mlp.fc1.bias\": 3072,\n",
      "  \"backbone.0.layers.3.blocks.0.mlp.fc2.weight\": 2359296,\n",
      "  \"backbone.0.layers.3.blocks.0.mlp.fc2.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.norm1.weight\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.norm1.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.relative_position_bias_table\": 4056,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.qkv.weight\": 1769472,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.qkv.bias\": 2304,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.proj.weight\": 589824,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.proj.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.norm2.weight\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.norm2.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.mlp.fc1.weight\": 2359296,\n",
      "  \"backbone.0.layers.3.blocks.1.mlp.fc1.bias\": 3072,\n",
      "  \"backbone.0.layers.3.blocks.1.mlp.fc2.weight\": 2359296,\n",
      "  \"backbone.0.layers.3.blocks.1.mlp.fc2.bias\": 768,\n",
      "  \"backbone.0.norm1.weight\": 192,\n",
      "  \"backbone.0.norm1.bias\": 192,\n",
      "  \"backbone.0.norm2.weight\": 384,\n",
      "  \"backbone.0.norm2.bias\": 384,\n",
      "  \"backbone.0.norm3.weight\": 768,\n",
      "  \"backbone.0.norm3.bias\": 768\n",
      "}\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:34,760 | \u001b[34mparams after freezing:\n",
      "{\n",
      "  \"transformer.level_embed\": 1024,\n",
      "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.0.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.0.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.0.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.1.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.1.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.1.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.2.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.2.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.2.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.3.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.3.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.3.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.4.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.4.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.4.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.5.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.5.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.5.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.0.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.0.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.0.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.0.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.0.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.0.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.0.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.0.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.0.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.0.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.0.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.0.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.1.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.1.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.1.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.1.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.1.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.1.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.1.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.1.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.1.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.1.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.1.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.1.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.2.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.2.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.2.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.2.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.2.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.2.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.2.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.2.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.2.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.2.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.2.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.2.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.3.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.3.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.3.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.3.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.3.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.3.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.3.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.3.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.3.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.3.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.3.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.3.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.4.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.4.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.4.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.4.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.4.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.4.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.4.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.4.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.4.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.4.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.4.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.4.norm2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.5.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.text_layers.5.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.text_layers.5.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.text_layers.5.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.5.linear1.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.5.linear1.bias\": 1024,\n",
      "  \"transformer.encoder.text_layers.5.linear2.weight\": 262144,\n",
      "  \"transformer.encoder.text_layers.5.linear2.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.5.norm1.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.5.norm1.bias\": 256,\n",
      "  \"transformer.encoder.text_layers.5.norm2.weight\": 256,\n",
      "  \"transformer.encoder.text_layers.5.norm2.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.0.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.1.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.2.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.3.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.4.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.gamma_v\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.gamma_l\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.layer_norm_v.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.layer_norm_v.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.layer_norm_l.weight\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.layer_norm_l.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.values_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.values_v_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.values_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.values_l_proj.bias\": 1024,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.out_v_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.out_v_proj.bias\": 256,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.out_l_proj.weight\": 262144,\n",
      "  \"transformer.encoder.fusion_layers.5.attn.out_l_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.0.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.0.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.0.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.1.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.1.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.1.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.2.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.2.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.2.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.3.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.3.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.3.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.4.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.4.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.4.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n",
      "  \"transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n",
      "  \"transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.ca_text.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.5.ca_text.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.5.ca_text.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.ca_text.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.catext_norm.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.catext_norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.5.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm3.bias\": 256,\n",
      "  \"transformer.decoder.norm.weight\": 256,\n",
      "  \"transformer.decoder.norm.bias\": 256,\n",
      "  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n",
      "  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n",
      "  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n",
      "  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n",
      "  \"transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n",
      "  \"transformer.tgt_embed.weight\": 230400,\n",
      "  \"transformer.enc_output.weight\": 65536,\n",
      "  \"transformer.enc_output.bias\": 256,\n",
      "  \"transformer.enc_output_norm.weight\": 256,\n",
      "  \"transformer.enc_output_norm.bias\": 256,\n",
      "  \"transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n",
      "  \"transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n",
      "  \"transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n",
      "  \"transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n",
      "  \"transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n",
      "  \"transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n",
      "  \"feat_map.weight\": 196608,\n",
      "  \"feat_map.bias\": 256,\n",
      "  \"input_proj.0.0.weight\": 49152,\n",
      "  \"input_proj.0.0.bias\": 256,\n",
      "  \"input_proj.0.1.weight\": 256,\n",
      "  \"input_proj.0.1.bias\": 256,\n",
      "  \"input_proj.1.0.weight\": 98304,\n",
      "  \"input_proj.1.0.bias\": 256,\n",
      "  \"input_proj.1.1.weight\": 256,\n",
      "  \"input_proj.1.1.bias\": 256,\n",
      "  \"input_proj.2.0.weight\": 196608,\n",
      "  \"input_proj.2.0.bias\": 256,\n",
      "  \"input_proj.2.1.weight\": 256,\n",
      "  \"input_proj.2.1.bias\": 256,\n",
      "  \"input_proj.3.0.weight\": 1769472,\n",
      "  \"input_proj.3.0.bias\": 256,\n",
      "  \"input_proj.3.1.weight\": 256,\n",
      "  \"input_proj.3.1.bias\": 256,\n",
      "  \"backbone.0.patch_embed.proj.weight\": 4608,\n",
      "  \"backbone.0.patch_embed.proj.bias\": 96,\n",
      "  \"backbone.0.patch_embed.norm.weight\": 96,\n",
      "  \"backbone.0.patch_embed.norm.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.norm1.weight\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.norm1.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.relative_position_bias_table\": 507,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.qkv.weight\": 27648,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.qkv.bias\": 288,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.proj.weight\": 9216,\n",
      "  \"backbone.0.layers.0.blocks.0.attn.proj.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.norm2.weight\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.norm2.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.0.mlp.fc1.weight\": 36864,\n",
      "  \"backbone.0.layers.0.blocks.0.mlp.fc1.bias\": 384,\n",
      "  \"backbone.0.layers.0.blocks.0.mlp.fc2.weight\": 36864,\n",
      "  \"backbone.0.layers.0.blocks.0.mlp.fc2.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.norm1.weight\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.norm1.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.relative_position_bias_table\": 507,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.qkv.weight\": 27648,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.qkv.bias\": 288,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.proj.weight\": 9216,\n",
      "  \"backbone.0.layers.0.blocks.1.attn.proj.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.norm2.weight\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.norm2.bias\": 96,\n",
      "  \"backbone.0.layers.0.blocks.1.mlp.fc1.weight\": 36864,\n",
      "  \"backbone.0.layers.0.blocks.1.mlp.fc1.bias\": 384,\n",
      "  \"backbone.0.layers.0.blocks.1.mlp.fc2.weight\": 36864,\n",
      "  \"backbone.0.layers.0.blocks.1.mlp.fc2.bias\": 96,\n",
      "  \"backbone.0.layers.0.downsample.reduction.weight\": 73728,\n",
      "  \"backbone.0.layers.0.downsample.norm.weight\": 384,\n",
      "  \"backbone.0.layers.0.downsample.norm.bias\": 384,\n",
      "  \"backbone.0.layers.1.blocks.0.norm1.weight\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.norm1.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.relative_position_bias_table\": 1014,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.qkv.weight\": 110592,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.qkv.bias\": 576,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.proj.weight\": 36864,\n",
      "  \"backbone.0.layers.1.blocks.0.attn.proj.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.norm2.weight\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.norm2.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.0.mlp.fc1.weight\": 147456,\n",
      "  \"backbone.0.layers.1.blocks.0.mlp.fc1.bias\": 768,\n",
      "  \"backbone.0.layers.1.blocks.0.mlp.fc2.weight\": 147456,\n",
      "  \"backbone.0.layers.1.blocks.0.mlp.fc2.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.norm1.weight\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.norm1.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.relative_position_bias_table\": 1014,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.qkv.weight\": 110592,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.qkv.bias\": 576,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.proj.weight\": 36864,\n",
      "  \"backbone.0.layers.1.blocks.1.attn.proj.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.norm2.weight\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.norm2.bias\": 192,\n",
      "  \"backbone.0.layers.1.blocks.1.mlp.fc1.weight\": 147456,\n",
      "  \"backbone.0.layers.1.blocks.1.mlp.fc1.bias\": 768,\n",
      "  \"backbone.0.layers.1.blocks.1.mlp.fc2.weight\": 147456,\n",
      "  \"backbone.0.layers.1.blocks.1.mlp.fc2.bias\": 192,\n",
      "  \"backbone.0.layers.1.downsample.reduction.weight\": 294912,\n",
      "  \"backbone.0.layers.1.downsample.norm.weight\": 768,\n",
      "  \"backbone.0.layers.1.downsample.norm.bias\": 768,\n",
      "  \"backbone.0.layers.2.blocks.0.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.0.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.0.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.0.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.0.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.0.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.1.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.1.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.1.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.1.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.1.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.2.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.2.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.2.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.2.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.2.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.3.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.3.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.3.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.3.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.3.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.4.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.4.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.4.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.4.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.4.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.norm1.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.norm1.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.relative_position_bias_table\": 2028,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.qkv.weight\": 442368,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.qkv.bias\": 1152,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.proj.weight\": 147456,\n",
      "  \"backbone.0.layers.2.blocks.5.attn.proj.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.norm2.weight\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.norm2.bias\": 384,\n",
      "  \"backbone.0.layers.2.blocks.5.mlp.fc1.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.5.mlp.fc1.bias\": 1536,\n",
      "  \"backbone.0.layers.2.blocks.5.mlp.fc2.weight\": 589824,\n",
      "  \"backbone.0.layers.2.blocks.5.mlp.fc2.bias\": 384,\n",
      "  \"backbone.0.layers.2.downsample.reduction.weight\": 1179648,\n",
      "  \"backbone.0.layers.2.downsample.norm.weight\": 1536,\n",
      "  \"backbone.0.layers.2.downsample.norm.bias\": 1536,\n",
      "  \"backbone.0.layers.3.blocks.0.norm1.weight\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.norm1.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.relative_position_bias_table\": 4056,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.qkv.weight\": 1769472,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.qkv.bias\": 2304,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.proj.weight\": 589824,\n",
      "  \"backbone.0.layers.3.blocks.0.attn.proj.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.norm2.weight\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.norm2.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.0.mlp.fc1.weight\": 2359296,\n",
      "  \"backbone.0.layers.3.blocks.0.mlp.fc1.bias\": 3072,\n",
      "  \"backbone.0.layers.3.blocks.0.mlp.fc2.weight\": 2359296,\n",
      "  \"backbone.0.layers.3.blocks.0.mlp.fc2.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.norm1.weight\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.norm1.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.relative_position_bias_table\": 4056,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.qkv.weight\": 1769472,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.qkv.bias\": 2304,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.proj.weight\": 589824,\n",
      "  \"backbone.0.layers.3.blocks.1.attn.proj.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.norm2.weight\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.norm2.bias\": 768,\n",
      "  \"backbone.0.layers.3.blocks.1.mlp.fc1.weight\": 2359296,\n",
      "  \"backbone.0.layers.3.blocks.1.mlp.fc1.bias\": 3072,\n",
      "  \"backbone.0.layers.3.blocks.1.mlp.fc2.weight\": 2359296,\n",
      "  \"backbone.0.layers.3.blocks.1.mlp.fc2.bias\": 768,\n",
      "  \"backbone.0.norm1.weight\": 192,\n",
      "  \"backbone.0.norm1.bias\": 192,\n",
      "  \"backbone.0.norm2.weight\": 384,\n",
      "  \"backbone.0.norm2.bias\": 384,\n",
      "  \"backbone.0.norm3.weight\": 768,\n",
      "  \"backbone.0.norm3.bias\": 768\n",
      "}\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-11-16 21:45:34,763 | \u001b[34mbuild dataset ... ...\u001b[0m\n",
      "/home/km/content/xray_data/xray_data/train /home/km/content/input_params/train.jsonl /home/km/content/input_params/label.json\n",
      "  == total images: 4379\n",
      "  == total labels: 14\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-11-16 21:45:34,849 | \u001b[34mbuild dataset, done.\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-11-16 21:45:34,849 | \u001b[34mnumber of training dataset: 1, samples: 4379\u001b[0m\n",
      "/home/km/content/xray_data/xray_data/test /home/km/content/xray_data/xray_data/test/_annotations.coco.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "/home/km/content/Open-GroundingDino/main.py:240: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.pretrain_model_path, map_location='cpu')['model']\n",
      "\u001b[32mINFO    \u001b[0m \u001b[32m2024-11-16 21:45:35,093 | \u001b[34mIgnore keys: []\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/km/content/Open-GroundingDino/main.py\", line 372, in <module>\n",
      "    main(args)\n",
      "  File \"/home/km/content/Open-GroundingDino/main.py\", line 255, in main\n",
      "    _load_output = model_without_ddp.load_state_dict(_tmp_st, strict=False)\n",
      "  File \"/home/km/pytorch_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Error(s) in loading state_dict for GroundingDINO:\n",
      "\tsize mismatch for input_proj.0.0.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 192, 1, 1]).\n",
      "\tsize mismatch for input_proj.1.0.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 384, 1, 1]).\n",
      "\tsize mismatch for input_proj.2.0.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 768, 1, 1]).\n",
      "\tsize mismatch for input_proj.3.0.weight: copying a param with shape torch.Size([256, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 768, 3, 3]).\n",
      "\tsize mismatch for backbone.0.patch_embed.proj.weight: copying a param with shape torch.Size([128, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([96, 3, 4, 4]).\n",
      "\tsize mismatch for backbone.0.patch_embed.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.patch_embed.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.patch_embed.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 4]) from checkpoint, the shape in current model is torch.Size([169, 3]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.attn.qkv.weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([288, 96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.attn.qkv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([288]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.attn.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.attn.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.mlp.fc1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([384, 96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.mlp.fc1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([96, 384]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 4]) from checkpoint, the shape in current model is torch.Size([169, 3]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.attn.qkv.weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([288, 96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.attn.qkv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([288]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.attn.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.attn.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.mlp.fc1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([384, 96]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.mlp.fc1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([96, 384]).\n",
      "\tsize mismatch for backbone.0.layers.0.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "\tsize mismatch for backbone.0.layers.0.downsample.reduction.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([192, 384]).\n",
      "\tsize mismatch for backbone.0.layers.0.downsample.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.0.downsample.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 8]) from checkpoint, the shape in current model is torch.Size([169, 6]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.attn.qkv.weight: copying a param with shape torch.Size([768, 256]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.attn.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.attn.proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.attn.proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.mlp.fc1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.mlp.fc1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 8]) from checkpoint, the shape in current model is torch.Size([169, 6]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.attn.qkv.weight: copying a param with shape torch.Size([768, 256]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.attn.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.attn.proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.attn.proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.mlp.fc1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.mlp.fc1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n",
      "\tsize mismatch for backbone.0.layers.1.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.layers.1.downsample.reduction.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n",
      "\tsize mismatch for backbone.0.layers.1.downsample.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.1.downsample.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.attn.qkv.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.attn.proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.attn.proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.mlp.fc1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.mlp.fc1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.attn.qkv.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.attn.proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.attn.proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.mlp.fc1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.mlp.fc1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.attn.qkv.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.attn.proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.attn.proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.mlp.fc1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.mlp.fc1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.mlp.fc2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.2.mlp.fc2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.attn.qkv.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.attn.proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.attn.proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.mlp.fc1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.mlp.fc1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.mlp.fc2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.3.mlp.fc2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.attn.qkv.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.attn.proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.attn.proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.mlp.fc1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.mlp.fc1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.mlp.fc2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.4.mlp.fc2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.attn.qkv.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.attn.proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.attn.proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.mlp.fc1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.mlp.fc1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.mlp.fc2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.blocks.5.mlp.fc2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.layers.2.downsample.reduction.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([768, 1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.downsample.norm.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
      "\tsize mismatch for backbone.0.layers.2.downsample.norm.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.norm1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.norm1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 32]) from checkpoint, the shape in current model is torch.Size([169, 24]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.attn.qkv.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.attn.qkv.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([2304]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.attn.proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.attn.proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.norm2.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.norm2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.mlp.fc1.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.mlp.fc1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([3072]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([1024, 4096]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.norm1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.norm1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([529, 32]) from checkpoint, the shape in current model is torch.Size([169, 24]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.attn.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.attn.qkv.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.attn.qkv.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([2304]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.attn.proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.attn.proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.norm2.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.norm2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.mlp.fc1.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.mlp.fc1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([3072]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([1024, 4096]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
      "\tsize mismatch for backbone.0.layers.3.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "\tsize mismatch for backbone.0.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for backbone.0.norm3.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for backbone.0.norm3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n"
     ]
    }
   ],
   "source": [
    "%cd /home/km/content/Open-GroundingDino\n",
    "GPU_NUM=1\n",
    "CGF=\"/home/km/content/Open-GroundingDino/config/cfg_odvg.py\"\n",
    "DATASETS=\"/home/km/content/Open-GroundingDino/config/datasets_mixed_odvg.json\"\n",
    "OUTPUT_DIR=\"/home/km/content/output_base\"\n",
    "!chmod +x train_dist.sh\n",
    "!bash train_dist.sh {CGF} {DATASETS} {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217620db-e371-40f1-bdff-4bfda615a2e5",
   "metadata": {},
   "source": [
    "# 7. Inference on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc04dd5-1cfa-44c9-881d-37fe18e2db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/km/content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbb3ef-62cf-47fd-93d2-eee8fe834e32",
   "metadata": {},
   "source": [
    "## Modifying inference_on_a_image.py to Save Predictions to JSON Files\n",
    "\n",
    "This code modifies the inference_on_a_image.py script to save predictions to JSON files without manually editing the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115cd1a-ab03-4c30-aed9-e9213890bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/home/km/content/allval_images_in_folder_base\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfddb5d9-6d55-4062-a7d9-7d65a7083c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/km/content/allval_images_in_folder_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387aedf-8aa3-4e4a-a101-ca975e7a5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5fd5c-8d59-4dd7-bc12-c21735067adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"/home/km/content/Open-GroundingDino/tools/inference_on_a_image.py\" \\\n",
    "  -c \"/home/km/content/Open-GroundingDino/tools/GroundingDINO_SwinB_cfg.py\" \\\n",
    "  -p \"/home/km/content/output_base/checkpoint0014.pth\" \\\n",
    "  -i \"/home/km/content/xray_data/xray_data/valid/c4d68aad9fdec87b76853d26744026bb.jpg\" \\\n",
    "  -t \"nodule mass . pleural effusion . pleural thickening . pneumothorax . pulmonary fibrosis . other lesion \" \\\n",
    "  -o pred_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110aa97-3e4f-4f1f-b04f-640fcdd2b3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373c5cb-ba85-40ef-a0d3-953903cea64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2ef18-9068-4ede-a5af-2c64ca6a5c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
